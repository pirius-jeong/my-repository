# 9장. Learning and Adaptation(학습과 적응)

## 9.1 학습과 적응 패턴 개요

- **정의**: 에이전트가 **경험·데이터 기반으로 지식·행동·전략을 자율 개선**하는 패턴이다.
- **왜 필요?** 정적 에이전트(사전 프로그래밍) 한계: 새로운 상황·개선 불가 → 학습으로 **진화형 에이전트** 구현.
- **핵심**: 경험 → 피드백 → 적응 → 성능 향상 루프.

### 큰 그림: 학습 유형

| 유형 | 특징 | 예시 에이전트 |
|------|------|--------------|
| **강화 학습(RL)** | 보상/패널티로 최적 행동 학습 | 로봇 제어, 게임 AI |
| **지도 학습** | 레이블 데이터로 패턴 학습 | 이메일 분류, 추세 예측 |
| **비지도 학습** | 레이블 없이 패턴 발견 | 데이터 탐색, 클러스터링 |
| **Few/Zero-Shot (LLM)** | 예시·지침으로 즉시 적응 | 새로운 작업 신속 처리 |
| **온라인 학습** | 실시간 데이터로 지속 업데이트 | 데이터 스트림 처리 |
| **기억 기반** | 과거 경험 회상·적용 | 맥락 인식 의사결정 |

---

## 9.2 주요 기법

### 9.2.1 PPO(Proximal Policy Optimization)

- **강화 학습 알고리즘**: 연속 행동 공간(로봇 관절, 게임) 최적화.
- **작동**:  
  1. 현재 정책으로 경험 수집(상태·행동·보상).  
  2. “클리핑된” 목적 함수로 정책 업데이트(안전 영역 내).  
  3. 급격 변화 방지 → 안정적 학습.
- **장점**: 기존 PPO보다 안정적·효율적.

### 9.2.2 DPO(Direct Preference Optimization)

- **LLM 정렬 기법**: 인간 선호도 데이터로 직접 최적화.
- **PPO 대비**:  
  | PPO (2단계) | DPO (직접) |
  |-------------|------------|
  | 1) 보상 모델 학습<br>2) PPO 미세조정 | 선호도 데이터 → 정책 직접 업데이트 |
- **장점**: 보상 모델 생략 → 간단·안정·효율 ↑, 해킹 위험 ↓.

---

## 9.3 응용 사례

- **개인화 비서**: 사용자 행동 분석 → 응답 최적화.
- **트레이딩 봇**: 실시간 시장 데이터 → 매매 전략 적응.
- **앱 에이전트**: 사용자 행동 → UI·기능 동적 수정.
- **로봇·자율주행**: 센서·과거 데이터 → 탐색·대응 개선.
- **사기 탐지**: 새 패턴 학습 → 탐지 정확도 ↑.
- **추천 시스템**: 선호도 학습 → 개인화 추천.
- **게임 AI**: 플레이어 행동 → 난이도·전략 조정.
- **지식 기반**: RAG로 경험 저장·회상(14장).

---

## 9.4 사례 연구

### 9.4.1 SICA(Self-Improving Coding Agent)

- **자체 개선 코딩 에이전트**: **자신의 소스 코드 수정**으로 진화.
- **루프**:  
  1. 과거 버전·벤치마크 분석 → 최고 성능 버전 선택.  
  2. 아카이브 분석 → 개선 수정 제안·적용.  
  3. 수정 버전 테스트 → 결과 저장 → 반복.
- **발전 예**:  
  - 기본 파일 덮어쓰기 → Smart Editor(AST 기반).  
  - 검색 → 하이브리드 심볼 로케이터.  
- **아키텍처**: 파일 작업·명령 실행·하위 에이전트(코딩·문제 해결) + 비동기 감독자.
- **보안**: Docker 격리, 관찰 가능성(콜그래프).

### 9.4.2 AlphaEvolve (Google)

- **알고리즘 발견·최적화 에이전트**: Gemini + 진화 알고리즘.
- **성과**:  
  - 데이터 센터 일정 0.7% 개선.  
  - TPU Verilog 최적화.  
  - Gemini 커널 23% 속도 ↑.  
  - 행렬 곱셈 새 알고리즘 발견.
- **학술**: 75% 미해결 문제 재발견, 20% 개선.

### 9.4.3 OpenEvolve

- **코드 진화 에이전트**: 전체 파일 최적화, 다언어·다LLM 지원.
- **구성**: 컨트롤러 + 프로그램 샘플러 + 평가자 풀 + LLM 앙상블.

---

## 9.5 한눈에 보기

- **무엇**: **경험 기반 자율 개선**으로 정적 → 동적 에이전트 변환.
- **왜**: 변화 환경에서 지속 성능 최적화·개선 필요.
- **어떻게**:  
  - PPO·DPO 등 ML 기법.  
  - SICA·AlphaEvolve 같은 자체 개선 시스템.
- **언제**: 개인화·실시간 적응·자동 최적화 필요 시.

---

## 9.6 핵심 정리

- 학습·적응은 **에이전트의 진정한 자율성**을 부여한다.
- PPO·DPO 등 기법 + SICA·AlphaEvolve 사례로 실현 가능성 입증.
- 메모리·다중 에이전트 등 이전 패턴과 결합 시 시너지 극대화.
